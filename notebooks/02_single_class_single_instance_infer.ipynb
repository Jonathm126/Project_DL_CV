{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.io\n",
    "\n",
    "# paths\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# set paths\n",
    "dirpath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(dirpath)\n",
    "\n",
    "# my imports\n",
    "from models.SoSi_detection import SoSiDetectionModel\n",
    "from utils.plot_utils import voc_img_bbox_plot\n",
    "\n",
    "# the lifesaver\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# torch setup\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_path = 'models\\\\model_savepoints\\\\'\n",
    "model_file = 'p1_model_a_1.pth'\n",
    "model_path = os.path.join(dirpath, model_path, model_file)\n",
    "\n",
    "# build and load model\n",
    "model = SoSiDetectionModel()  \n",
    "sucess = model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(sucess)\n",
    "model.to(device).eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_transform = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train, backbone_transforms):\n",
    "    transforms = []    \n",
    "    # standard transforms - resizing and center cropping for 1:1 aspect ratio and 224 size\n",
    "    transforms.append(T.Resize(size = backbone_transforms.resize_size, interpolation = backbone_transforms.interpolation))\n",
    "    transforms.append(T.CenterCrop(size=backbone_transforms.crop_size))\n",
    "    \n",
    "    # if training mode, add flips and jitters\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))\n",
    "    \n",
    "    # standard transforms - normalizing\n",
    "    transforms.append(T.ToImage())\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True)) # scale to 0-1\n",
    "    # TODO normalization makes everything wierd\n",
    "    transforms.append(T.Normalize(mean = backbone_transforms.mean, std = backbone_transforms.std))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define preprocessing\n",
    "def preprocess_frame(frame, transform):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = transform(image)  # Apply transformations\n",
    "    return image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Define postprocessing\n",
    "def postprocess_predictions(pred_bboxes, pred_labels, threshold=0.5):\n",
    "    pred_labels = torch.sigmoid(pred_labels)  # Convert logits to probabilities\n",
    "    pred_labels = (pred_labels > threshold).long()  # Apply threshold\n",
    "\n",
    "    # Convert bounding boxes (if necessary)\n",
    "    pred_bboxes = box_convert(pred_bboxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\")  # Convert (if needed)\n",
    "\n",
    "    return pred_bboxes.cpu().numpy(), pred_labels.cpu().numpy()\n",
    "\n",
    "# Draw bounding boxes on a frame\n",
    "def draw_predictions(frame, boxes, labels):\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = map(int, box)  # Convert to int\n",
    "        color = (0, 255, 0) if label == 1 else (0, 0, 255)  # Green for class 1, Red otherwise\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f\"Class: {label}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return frame\n",
    "\n",
    "# Run inference on video\n",
    "def infer_on_video(model_path, video_path, output_path, device=\"cuda\"):\n",
    "    model = load_model(model_path, device)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Define preprocessing transform\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize((224, 224)),  # Adjust to match model input size\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Adjust for your model\n",
    "    ])\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Preprocess frame\n",
    "        input_tensor = preprocess_frame(frame, transform).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            pred_bboxes, pred_labels = model(input_tensor)\n",
    "\n",
    "        # Postprocess predictions\n",
    "        boxes, labels = postprocess_predictions(pred_bboxes, pred_labels)\n",
    "\n",
    "        # Draw predictions on frame\n",
    "        output_frame = draw_predictions(frame, boxes, labels)\n",
    "\n",
    "        # Write to output video\n",
    "        out.write(output_frame)\n",
    "\n",
    "        # Display frame (optional)\n",
    "        cv2.imshow(\"Inference\", output_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to stop\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "infer_on_video(\"model_weights.pth\", \"input_video.mp4\", \"output_video.avi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_CatSemSeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
